{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "import typing\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import ImgObsWrapper\n",
    "from gym_minigrid.envs.numbertasks import NumberTaskType\n",
    "\n",
    "from stable_baselines3 import A2C, PPO, DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_env(env_id: str, seed: int, **kwargs):\n",
    "    env = gym.make(env_id, seed=seed, **kwargs)\n",
    "    return Monitor(ImgObsWrapper(env))\n",
    "\n",
    "\n",
    "def _make_n_envs(env_id: str, n: int, seed: int, normalize: bool = True, **kwargs):\n",
    "    env = DummyVecEnv([lambda: _make_env(env_id, seed=seed + i, **kwargs) for i in range(n)])\n",
    "    if normalize:\n",
    "        return VecNormalize(env)\n",
    "\n",
    "    else:\n",
    "        return env\n",
    "\n",
    "\n",
    "def _make_single_env(env_id: str, seed: int, **kwargs):\n",
    "    return _make_n_envs(env_id, 1, seed, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 36.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 36.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 36.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 36.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 36.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 36.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 36.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=0.57 +/- 0.47\n",
      "Episode length: 4.04 +/- 8.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=0.46 +/- 0.47\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=0.47 +/- 0.48\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=0.47 +/- 0.47\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=0.47 +/- 0.47\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=0.40 +/- 0.47\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=0.40 +/- 0.47\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=0.46 +/- 0.47\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=0.38 +/- 0.47\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=0.47 +/- 0.47\n",
      "Episode length: 2.68 +/- 4.76\n",
      "Eval num_timesteps=180000, episode_reward=0.55 +/- 0.47\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=0.51 +/- 0.47\n",
      "Episode length: 2.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=0.47 +/- 0.48\n",
      "Episode length: 2.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "N_ENVS = 1\n",
    "N_EVALS = 20\n",
    "N_BASE_STEPS = 200000\n",
    "N_TOTAL_STEPS = N_BASE_STEPS * N_ENVS\n",
    "N_EVAL_EPISODES = 50\n",
    "\n",
    "ENV_ID = 'MiniGrid-NumberTasksNosePoke-v0'\n",
    "TRAIN_SEED = 100\n",
    "EVAL_SEED = 200\n",
    "\n",
    "# TASKS = NumberTaskType\n",
    "TASKS = [NumberTaskType.color]\n",
    "\n",
    "AGENT_TYPES = [DQN]  #, PPO]\n",
    "AGENT_KWARGS = dict()  #  dict(learning_rate=1e-3) #\n",
    "\n",
    "ENV_KWARGS = dict() # dict(shuffle_task_locations=True) #, min_agent_view_size=5, step_reward=-0.01)\n",
    "\n",
    "all_agent_results = defaultdict(dict)\n",
    "\n",
    "\n",
    "for task in TASKS:\n",
    "    for agent_type in AGENT_TYPES:\n",
    "        if N_ENVS == 1:\n",
    "            env = _make_single_env(ENV_ID, seed=TRAIN_SEED, task=task, **ENV_KWARGS)\n",
    "        else:\n",
    "            env = _make_n_envs(ENV_ID, n=N_ENVS, seed=TRAIN_SEED, task=task, **ENV_KWARGS)\n",
    "\n",
    "        eval_env =_make_single_env(ENV_ID, seed=EVAL_SEED, task=task, **ENV_KWARGS)\n",
    "        stop_callback = StopTrainingOnRewardThreshold(reward_threshold=0.85, verbose=1)\n",
    "        eval_callback = EvalCallback(eval_env, n_eval_episodes=N_EVAL_EPISODES, \n",
    "            eval_freq=N_BASE_STEPS / N_EVALS, callback_on_new_best=stop_callback,\n",
    "            log_path=f'./logs/{agent_type.__name__}_{task.name}', verbose=1)\n",
    "\n",
    "        model = agent_type('MlpPolicy', env,  **AGENT_KWARGS)\n",
    "        model.learn(total_timesteps=N_TOTAL_STEPS, callback=eval_callback)\n",
    "\n",
    "        rewards = eval_callback.evaluations_results\n",
    "        means = [np.mean(r) for r in rewards]\n",
    "        stds = [np.std(r) for r in rewards]\n",
    "\n",
    "        all_agent_results[task.name][agent_type.__name__] = means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.57, 0.4654030511288038)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, eval_env, n_eval_episodes=N_EVAL_EPISODES * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_vec_normalize_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color A2C [0.0, 0.5700000000000001, 0.9200000000000002]\n",
      "color PPO [0.665, 0.9499999999999998]\n",
      "magnitude A2C [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19, 0.0, 0.095, 0.095, 0.0, 0.0, 0.0, 0.0, 0.095, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "magnitude PPO [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "parity A2C [0.38, 0.475, 0.47000000000000003, 0.475, 0.655, 0.38, 0.665, 0.665, 0.38, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "parity PPO [0.38, 0.5700000000000001, 0.475, 0.38, 0.38, 0.5700000000000001, 0.665, 0.5700000000000001, 0.475, 0.38, 0.19, 0.19, 0.285, 0.475, 0.5700000000000001, 0.38, 0.285, 0.0, 0.19, 0.095]\n"
     ]
    }
   ],
   "source": [
    "for task_name, agent_results in all_agent_results.items():\n",
    "    for agent, results in agent_results.items():\n",
    "        means, stds = results\n",
    "        print(task_name, agent, means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2,  5,  0],\n",
       "        [ 2,  5,  0],\n",
       "        [ 8,  1,  0]],\n",
       "\n",
       "       [[ 2,  5,  0],\n",
       "        [11,  1,  5],\n",
       "        [ 1,  0,  0]],\n",
       "\n",
       "       [[ 2,  5,  0],\n",
       "        [ 2,  5,  0],\n",
       "        [ 8,  1,  0]]], dtype=uint8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = _make_env(ENV_ID, seed=TRAIN_SEED, task='color')\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 2,  5,  0],\n",
       "         [ 2,  5,  0],\n",
       "         [11,  1,  5]],\n",
       " \n",
       "        [[ 6,  2,  0],\n",
       "         [ 8,  1,  0],\n",
       "         [ 1,  0,  0]],\n",
       " \n",
       "        [[ 2,  5,  0],\n",
       "         [ 2,  5,  0],\n",
       "         [ 2,  5,  0]]], dtype=uint8),\n",
       " 0,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.actions.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 2,  5,  0],\n",
       "         [ 2,  5,  0],\n",
       "         [ 2,  5,  0]],\n",
       " \n",
       "        [[11,  5,  0],\n",
       "         [ 8,  1,  0],\n",
       "         [ 1,  0,  0]],\n",
       " \n",
       "        [[ 2,  5,  0],\n",
       "         [ 2,  5,  0],\n",
       "         [11,  1,  5]]], dtype=uint8),\n",
       " 0,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.actions.left)\n",
    "env.step(env.actions.left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9, 0.9, 0.95, 0.95, 0.9, 0.9, 0.9, 0.95, 0.9, 0.95]\n"
     ]
    }
   ],
   "source": [
    "N_ITER = 10\n",
    "\n",
    "env = _make_env(ENV_ID, seed=TRAIN_SEED, task='parity')\n",
    "rewards = []\n",
    "\n",
    "for _ in range(N_ITER):\n",
    "    obs = env.reset()\n",
    "    digit = obs[1, 1, -1]\n",
    "    turn_obs, _, _, _ = env.step(env.actions.left)\n",
    "    task_marker = turn_obs[1, 0, -1]\n",
    "\n",
    "    if (digit % 2) != task_marker:\n",
    "        env.step(env.actions.left)\n",
    "        env.step(env.actions.left)\n",
    "\n",
    "    _, r, done, _ = env.step(env.actions.forward)\n",
    "    if not done:\n",
    "        raise ValueError('Expected done')\n",
    "    rewards.append(r)\n",
    "\n",
    "print(rewards)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95, 0.9, 0.9, 0.9, 0.95, 0.95, 0.9, 0.9, 0.9, 0.9]\n"
     ]
    }
   ],
   "source": [
    "N_ITER = 10\n",
    "\n",
    "env = _make_env(ENV_ID, seed=TRAIN_SEED, task='magnitude')\n",
    "rewards = []\n",
    "\n",
    "for _ in range(N_ITER):\n",
    "    obs = env.reset()\n",
    "    digit = obs[1, 1, -1]\n",
    "    turn_obs, _, _, _ = env.step(env.actions.left)\n",
    "    task_marker = turn_obs[1, 0, -1]\n",
    "\n",
    "    if (digit >= 5) != (task_marker == 11):\n",
    "        env.step(env.actions.left)\n",
    "        env.step(env.actions.left)\n",
    "\n",
    "    _, r, done, _ = env.step(env.actions.forward)\n",
    "    if not done:\n",
    "        raise ValueError('Expected done')\n",
    "    rewards.append(r)\n",
    "\n",
    "print(rewards)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95, 0.95, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 0.95, 0.95]\n"
     ]
    }
   ],
   "source": [
    "N_ITER = 10\n",
    "\n",
    "env = _make_env(ENV_ID, seed=TRAIN_SEED, task='color')\n",
    "rewards = []\n",
    "\n",
    "for _ in range(N_ITER):\n",
    "    obs = env.reset()\n",
    "    digit_color = obs[1, 1, 1]\n",
    "    turn_obs, _, _, _ = env.step(env.actions.left)\n",
    "    task_color = turn_obs[1, 0, 1]\n",
    "\n",
    "    if digit_color != task_color:\n",
    "        env.step(env.actions.left)\n",
    "        env.step(env.actions.left)\n",
    "\n",
    "    _, r, done, _ = env.step(env.actions.forward)\n",
    "    if not done:\n",
    "        raise ValueError('Expected done')\n",
    "    rewards.append(r)\n",
    "\n",
    "print(rewards)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8775025b178470b7b487df744aa50e287915f8de1ad29ac834985f09f2d3ff0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('minigrid')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
